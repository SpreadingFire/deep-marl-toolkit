
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: test1371
  name: ray-worker-test2
spec:
  # Change this to scale the number of worker nodes started in the Ray cluster.
  replicas: 2
  selector:
    matchLabels:
      component: ray-worker-test2
      type: ray1-test2
  template:
    metadata:
      labels:
        component: ray-worker-test2
        type: ray1-test2
    spec:
      restartPolicy: Always
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      containers:
      - name: ray-worker-test2
        image: 10.18.10.10:5000/xiaobr/other/ray_36:v0.0.8m
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c", "--"]
        args:
          - "ray start --num-cpus=$MY_CPU_REQUEST --address=$RAY_HEAD_TEST2_SERVICE_HOST:$RAY_HEAD_TEST2_SERVICE_PORT_REDIS --object-manager-port=12345 --node-manager-port=12346 --block"
        # This volume allocates shared memory for Ray to use for its plasma
        # object store. If you do not provide this, Ray will fall back to
        # /tmp which cause slowdowns if is not a shared memory volume.
        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
        env:
          # This is used in the ray start command so that Ray can spawn the
          # correct number of processes. Omitting this may lead to degraded
          # performance.
          - name: MY_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                resource: requests.cpu
        resources:
          requests:
            cpu: 2000m
            memory: 1600Mi
          limits:
            cpu: 2000m
